{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnAvAJ6Y9s0ywbB6xov68E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brettin/llm_tutorial/blob/main/tutorials/02-prompt-engineering/ToT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLG0Zt7PNWjY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install haystack-ai transformers boilerpy3 sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "Z1YNx62O_g1S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from haystack.components.generators import HuggingFaceTGIGenerator\n",
        "os.environ[\"HF_API_TOKEN\"] = getpass(\"Enter Hugging Face token: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ez8NTzsTAYfK",
        "outputId": "e639b63a-3dc3-4bed-e842-43f6b6a5a634"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Hugging Face token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the install by creating a prompt.\n",
        "\n",
        "# Create a prompt.\n",
        "question = '''Please provide a detailed description of the biochemical function\n",
        "5-(hydroxymethyl)furfural/furfural transporter.'''\n",
        "\n",
        "prompt = f'''Imagine three different experts are answering this question.\n",
        "\n",
        "In the first step, each experts will write down 1 answer and share it with the\n",
        "group for evaluation. After the first step each expert will evaluate\n",
        "the answers of the other experts. All experts must present their answer before\n",
        "answers are evaluated.\n",
        "\n",
        "If, after any step, two experts disagree with the other expert's answer,\n",
        "then that expert retracts its answer and leaves the conversation.\n",
        "\n",
        "This pattern of sharing answers followed by evaluation of the answers contines\n",
        "for two more steps.\n",
        "\n",
        "When the experts are done, please compile the final answer from the remaining experts and label it as FINAL ANSWER.\n",
        "The question is: {question}\n",
        "'''\n",
        "\n",
        "generator = HuggingFaceTGIGenerator(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "generator.warm_up()\n",
        "result = generator.run(prompt=prompt, generation_kwargs={\"max_new_tokens\": 1024})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GCh08jKXYtYy",
        "outputId": "a15d3d8b-b08d-4ebb-8038-62b76c2b557e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['replies'][0])"
      ],
      "metadata": {
        "id": "pOtMEDP4FKAP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}