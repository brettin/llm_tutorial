{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdG2Qw/E13Y82FzlkG4I9A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brettin/llm_tutorial/blob/main/tutorials/05-chains/05_chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the environment"
      ],
      "metadata": {
        "id": "jfjSiaw5QD2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install langchain\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "2O-0LfWlP5SV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d790a6dd-e803-4107-cb0d-6156d2f571b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.343)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-core<0.1,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.7)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.67)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import warnings\n",
        "from getpass import getpass\n",
        "from langchain.llms import HuggingFaceHub, OpenAI\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#token='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'\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = getpass('Enter HUGGINGFACEHUB_API_TOKEN :')\n",
        "os.environ['OPENAI_API_KEY'] = getpass('Enter OPENAI_API_KEY:')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "190-5TLHQk6D",
        "outputId": "efce45df-6ebc-4002-9cdd-c5b8ae57200c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter HUGGINGFACEHUB_API_TOKEN :··········\n",
            "Enter OPENAI_API_KEY:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "falcon = HuggingFaceHub(\n",
        "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
        "    model_kwargs={\"temperature\": 0.5, \"max_length\": 256}\n",
        ")\n",
        "\n",
        "open_ai = OpenAI(temperature =0.5, max_length = 128)\n",
        "\n",
        "print (open_ai.model_name)\n",
        "print (open_ai.model_kwargs)\n",
        "print (open_ai.temperature)\n",
        "print (falcon.model_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnX8rU734lgV",
        "outputId": "d8c76018-d025-4129-fe23-0a9dc6385ca0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text-davinci-003\n",
            "{'max_length': 128}\n",
            "0.5\n",
            "{'temperature': 0.5, 'max_length': 256}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ready Made Chains\n",
        "There are a number of prebuilt chainsIt's the prompt in the LLMMathChain. You could build this from scratch if you are good at prompt engineering.\n"
      ],
      "metadata": {
        "id": "StoSXFITPAlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMMathChain"
      ],
      "metadata": {
        "id": "xgxOVth-hGCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Q5BEwxrWOWD7",
        "outputId": "9434764c-7424-47ec-f733-73f459099db2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: 3000'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from langchain.chains import LLMMathChain\n",
        "\n",
        "llm_math = LLMMathChain.from_llm(falcon)\n",
        "\n",
        "question = \"What is 300 * 10?\"\n",
        "\n",
        "llm_math.run(question)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_math.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCd4XPTH6MLX",
        "outputId": "13bade26-9d71-4a8e-f522-fc7af87ebe7d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
            "\n",
            "Question: ${{Question with math problem.}}\n",
            "```text\n",
            "${{single line mathematical expression that solves the problem}}\n",
            "```\n",
            "...numexpr.evaluate(text)...\n",
            "```output\n",
            "${{Output of running the code}}\n",
            "```\n",
            "Answer: ${{Answer}}\n",
            "\n",
            "Begin.\n",
            "\n",
            "Question: What is 37593 * 67?\n",
            "```text\n",
            "37593 * 67\n",
            "```\n",
            "...numexpr.evaluate(\"37593 * 67\")...\n",
            "```output\n",
            "2518731\n",
            "```\n",
            "Answer: 2518731\n",
            "\n",
            "Question: 37593^(1/5)\n",
            "```text\n",
            "37593**(1/5)\n",
            "```\n",
            "...numexpr.evaluate(\"37593**(1/5)\")...\n",
            "```output\n",
            "8.222831614237718\n",
            "```\n",
            "Answer: 8.222831614237718\n",
            "\n",
            "Question: {question}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMRequestsChain\n",
        "This one is interesting in that the chain goes out and searches the web and looks for the answer."
      ],
      "metadata": {
        "id": "3sG6zpeFgoEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose we have a single-input chain that takes a 'question' string:\n",
        "# chain.run(\"What's the temperature in Boise, Idaho?\")\n",
        "# -> \"The temperature in Boise is...\"\n",
        "\n",
        "from langchain.chains import LLMRequestsChain, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "\n",
        "template = \"\"\"Between >>> and <<< are the raw search result text from google.\n",
        "Extract the answer to the question '{query}' or say \"not found\" if the information is not contained.\n",
        "Use the format\n",
        "Extracted:<answer or \"not found\">\n",
        ">>> {requests_result} <<<\n",
        "Extracted:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"requests_result\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "#chain = LLMRequestsChain(llm_chain=LLMChain(llm=falcon, prompt=prompt))\n",
        "chain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=.5), prompt=prompt))\n",
        "\n",
        "question = \"What are the Three (3) biggest countries, and their respective sizes?\"\n",
        "\n",
        "inputs = {\n",
        "    \"query\": question,\n",
        "    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n",
        "}\n",
        "chain(inputs)\n",
        "\n",
        "#question = \"What is the current temperature in Darien, IL?\"\n",
        "#inputs = {\n",
        "#    \"query\": question,\n",
        "#    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n",
        "#}\n",
        "#chain(inputs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXy5N0esSJHW",
        "outputId": "90d15c44-2b7a-406f-8689-e0c44092f814"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What are the Three (3) biggest countries, and their respective sizes?',\n",
              " 'url': 'https://www.google.com/search?q=What+are+the+Three+(3)+biggest+countries,+and+their+respective+sizes?',\n",
              " 'output': ' Russia (17.1M km²), Canada (10M km²), China (9.7M km²)'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SequentialChain, TransformChain, and LLMChain.\n",
        "\n",
        "In this example, we construct a chain that operates on the user input (TransformChain) and connect it to another chain that contains a prompt and an llm (LLMChain).\n",
        "\n",
        "We chain the TransformChain and the LLM Chain together usingf a SequentialChain. The Sequential Chain will pass the output of the TransformChain to the LLMChain!\n",
        "\n",
        "* The prompt is stored in the LLMChain.\n",
        "* The output of the TransformChain becomes the input to the LLMChain.\n",
        "* The SequentialChain\n",
        "\n",
        "Here is how we tie it together:\n",
        "\n",
        "1. Construct a prompt template.\n",
        "2. Instanciate the LLMChain with the prompt.\n",
        "3. Instanciate the TransformChain.\n",
        "4. Combine the TransformChain and the LLMChain\n",
        "\n",
        "The llm chain uses a prompt template, and that prompt template wants input in\n",
        "key value pairs. **The key name for the output from the TransformChain has to be the same as the key name for the input to the LLMChain**.\n",
        "\n",
        "One key being output_text and the second being style. So the transform chain needs to name it's output output_text."
      ],
      "metadata": {
        "id": "u9Fdw4oWmxGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain, TransformChain, SequentialChain\n",
        "\n",
        "\n",
        "import re\n",
        "def clean_input(inputs: dict) -> dict:\n",
        "  # assumes dict has a key 'text'\n",
        "  text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', inputs['text'])\n",
        "  text = re.sub(r'[ \\t]+', ' ', inputs['text'])\n",
        "  text = \"Think about this: \" + text\n",
        "  return {'transform_output_text': text}\n",
        "\n",
        "\n",
        "# 1. Construct a prompt template.\n",
        "template = \"\"\"Paraphrase this text:\n",
        "\n",
        "{transform_output_text}\n",
        "\n",
        "In the style of a {style}.\n",
        "\n",
        "Paraphrase: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"style\", \"transform_output_text\"], template=template)\n",
        "\n",
        "\n",
        "# 2. Instanciate the LLMChain with the prompt.\n",
        "llm_chain = LLMChain(llm=falcon, prompt=prompt, output_key='final_output')\n",
        "\n",
        "\n",
        "# 3. Instanciate the TransformChain.\n",
        "transform_chain = TransformChain(input_variables=['text'], output_variables=['transform_output_text'], transform=clean_input)\n",
        "\n",
        "\n",
        "# 4. Combine the TransformChain and the LLMChain\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[transform_chain, llm_chain],\n",
        "    input_variables=['text', 'style'],\n",
        "    output_variables=['final_output']\n",
        ")\n",
        "\n",
        "user_input = \"Hello world   from   my    home. Happy     Thanksgiving\"\n",
        "r = sequential_chain.run({'text': user_input, 'style': 'a 90s rapper'})\n",
        "print(r)\n",
        "\n",
        "for i, c in enumerate(sequential_chain.chains):\n",
        "  print(f\"\\nchain: {i}, input_keys: {c.input_keys}, output_keys: {c.output_keys}\")\n",
        "\n",
        "#print(llm_chain.prompt.format({'text': user_input, 'style': 'a 90s rapper'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSu0MLT9mSW0",
        "outputId": "6e036120-a965-48c5-92e8-8326b00f0478"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Yo, it's me, from my crib, saying hello to the world, happy Thanksgiving.\n",
            "\n",
            "chain: 0, input_keys: ['text'], output_keys: ['transform_output_text']\n",
            "\n",
            "chain: 1, input_keys: ['style', 'transform_output_text'], output_keys: ['final_output']\n"
          ]
        }
      ]
    }
  ]
}